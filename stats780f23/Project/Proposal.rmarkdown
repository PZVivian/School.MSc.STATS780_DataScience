---
title: |
  | STATS/CSE 780
  | Project Proposal
author: "Pao Zhu Vivian Hsu (Student Number: 400547994)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: pdf
editor: visual
execute:
  echo: false
  warning: false
  error: false
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
bibliography: Proposal.bib
csl: https://www.zotero.org/styles/apa-single-spaced
nocite: |
  @citeR
fontsize: 11pt
geometry: 
  - margin = 1in
linestretch: 1.5
---


\newpage

## Introduction
Diabetes is a chronic disease that occurs when the body cannot effectively produce or use insulin to regulate sugar levels in the blood. According to the World Health Organization, 422 million people have diabetes worldwide and 1.5 million deaths that occur every year are linked to the disease [@factsWHO]. In this paper, we propose a study that leverages machine learning techniques to predict the risk and prevent the onset of diabetes.

The data in this proposal was originally collected by Islam et al. from patients in Sylhet Diabetes Hospital in Sylhet, Bangladesh [-@diabetesStudy]. It was later openly published on Kaggle [@diabetesData], where it was downloaded. 

In Islam et al.'s study [-@diabetesStudy], machine learning techniques were used to predict diabetes risk. In particular, naive Bayes, logistic regression, and random forest techniques were applied. The study found that the random forest model had the best accuracy [@diabetesStudy]. Other studies have also applied machine learning to predict diabetes and varying methods were found to be most accurate. For example, Kumar and Velide compared seven different techniques and found that their J48(C4.5) model performed most accurately [-@litReview1] while Rabina and Chopra compared a decision tree with neural networks and found the decision tree to be more accurate [-@litReview2]. 

The study we propose will also utilize machine learning techniques to predict diabetes. It will focus on decision tree and neural network methods.


## Methods
The first machine learning method will be a decision tree. This method was selected because Islam et al.'s study  found that a decision tree model was most accurate [-@diabetesStudy] and so this study will aim to reproduce such results. The index on the subtrees $\alpha$ will be tuned using cross-validation. Pruning will be applied to reduce the cost complexity of the tree, and random forest ensembling will be used to improve model performance.

The second method will be a neural network. This method was selected since Islam et al.'s study has not explored a model using neural networks [-@diabetesStudy] and so there is a possibility that a neural network may be more accurate than a tree-based model. The number of hidden layers and the units per layer will be tuned through trial and error. Based on James et al.'s book on statistical learning [-@introToStatsLearning], the units per layer will be set to some large value and overfitting will be controlled with ridge regularization. The strength of the regularization $\lambda$ will be tuned at each layer.

After the two models are built, they will each be assessed using misclassification rate, accuracy, specificity, and sensitivity. A comparison of these four measurements will reveal which model is a stronger fit to predict diabetes. By nature, decision trees are easier to interpret and reproducible compared to neural networks. Thus, these qualities will also be considered in its comparison. 


## Preliminary Analysis

```{r setup}
library(knitr)
library(tidyverse)
library(corrplot)

```

```{r cleanse}
# ----- DATA CLEANSING ----- #
diabetes_raw <- read.csv("diabetes_data.csv", sep=";")
diabetes_int <- diabetes_raw %>% 
  mutate(gender = as.integer(ifelse(gender=="Male",1,
                            ifelse(gender=="Female",0,
                            NA))))
diabetes <- diabetes_raw %>% 
  mutate(gender = as.factor(ifelse(gender=="Male",1,
                            ifelse(gender=="Female",0,
                            NA))),
         polyuria = as.factor(polyuria),
         polydipsia = as.factor(polydipsia),
         sudden_weight_loss = as.factor(sudden_weight_loss),
         weakness = as.factor(weakness),
         polyphagia = as.factor(polyphagia),
         genital_thrush = as.factor(genital_thrush),
         visual_blurring = as.factor(visual_blurring),
         itching = as.factor(itching),
         irritability = as.factor(irritability),
         delayed_healing = as.factor(delayed_healing),
         partial_paresis = as.factor(partial_paresis),
         muscle_stiffness = as.factor(muscle_stiffness),
         alopecia = as.factor(alopecia),
         obesity = as.factor(obesity),
         class = as.factor(class)) 

```

```{r exp-desc, output=FALSE}
# ----- DATA EXPLORATION ----- #
# Data description
attribute <- c("Age","Gender","Polyuria","Polydipsia",
               "Sudden weight loss","Weakness","Polyphagia",
               "Genital thrush","Visual blurring","Itching",
               "Irritability","Delayed healing","Partial paresis",
               "Muscle stiffness","Alopecia","Obesity","Class")
values <- c("In years","1 = Male, 0 = Female","1 = Yes, 0 = No","1 = Yes, 0 = No",
            "1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No",
            "1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No",
            "1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No",
            "1 = Positive risk, 0 = Negative risk")
data_summary <- data.frame(Attribute=attribute, Values=values)
kable(data_summary)

```

```{r missing, output=FALSE}
# Check for missing data
nulls <- sapply(diabetes, 
                function(col){ifelse(is.na(sum(col == "")), 0, sum(col == ""))})
blanks <- sapply(diabetes, 
                 function(col){ifelse(is.na(sum(col == "")), 0, sum(col == ""))})
kable(data.frame(Nulls=sum(nulls), Blanks=sum(blanks)))

```

```{r boxplot, output=FALSE}
# Boxplot of continuous variable
bplot <- ggplot(diabetes, aes(y = age)) + geom_boxplot() + labs(x="",y="Age") +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
bplot_a1 <- as.integer(unlist(ggplot_build(bplot)$data)["ymax"])
bplot + geom_hline(yintercept = bplot_a1, linetype="dotted", color="red") + 
  geom_text(aes(-0.4,bplot_a1,label = bplot_a1, vjust = 1.5, color="red"), 
            show.legend = FALSE)

# Cap outliers
diabetes_no_outliers <- diabetes %>% mutate(age = ifelse(age > bplot_a1, bplot_a1, age))

```

```{r barchart-1, output=FALSE}
# Barplots for each categorical variable
ggplot(diabetes, aes(x = gender)) + geom_bar() + labs(y = "Count", x = "Gender")
```

```{r barchart-2, output=FALSE}
ggplot(diabetes, aes(x = polyuria)) + geom_bar() + labs(y = "Count", x = "Polyuria")
```

```{r barchart-3, output=FALSE}
ggplot(diabetes, aes(x = polydipsia)) + geom_bar() + 
  labs(y = "Count", x = "Polydipsia")
```

```{r barchart-4, output=FALSE}
ggplot(diabetes, aes(x = sudden_weight_loss)) + geom_bar() + 
  labs(y = "Count", x = "Sudden weight loss")
```

```{r barchart-5, output=FALSE}
ggplot(diabetes, aes(x = weakness)) + geom_bar() + labs(y = "Count", x = "Weakness")
```

```{r barchart-6, output=FALSE}
ggplot(diabetes, aes(x = polyphagia)) + geom_bar() + 
  labs(y = "Count", x = "Polyphagia")
```

```{r barchart-7, output=FALSE}
ggplot(diabetes, aes(x = genital_thrush)) + geom_bar() + 
  labs(y = "Count", x = "Genital thrush")
```

```{r barchart-8, output=FALSE}
ggplot(diabetes, aes(x = visual_blurring)) + geom_bar() + 
  labs(y = "Count", x = "Visual blurring")
```

```{r barchart-9, output=FALSE}
ggplot(diabetes, aes(x = itching)) + geom_bar() + labs(y = "Count", x = "Itching")
```

```{r barchart-10, output=FALSE}
ggplot(diabetes, aes(x = irritability)) + geom_bar() + 
  labs(y = "Count", x = "Irritability")
```

```{r barchart-11, output=FALSE}
ggplot(diabetes, aes(x = delayed_healing)) + geom_bar() + 
  labs(y = "Count", x = "Delayed healing")
```

```{r barchart-12, output=FALSE}
ggplot(diabetes, aes(x = partial_paresis)) + geom_bar() + 
  labs(y = "Count", x = "Partial paresis")
```

```{r barchart-13, output=FALSE}
ggplot(diabetes, aes(x = muscle_stiffness)) + geom_bar() + 
  labs(y = "Count", x = "Muscle stiffness")
```

```{r barchart-14, output=FALSE}
ggplot(diabetes, aes(x = alopecia)) + geom_bar() + labs(y = "Count", x = "Alopecia")
```

```{r barchart-15, output=FALSE}
ggplot(diabetes, aes(x = obesity)) + geom_bar() + labs(y = "Count", x = "Obesity")
```

```{r barchart-16, output=FALSE}
ggplot(diabetes, aes(x = class)) + geom_bar() + labs(y = "Count", x = "Class")

```

```{r corr, output=FALSE}
# Correlation plot
corr_matrix <- cor(diabetes_int)
corrplot(round(corr_matrix,2), method = "number", number.cex=0.75)
```



This section involves an exploratory analysis to provide insight on the data prior to machine learning.

The data set consists of `r nrow(diabetes)` observations and `r ncol(diabetes)` attributes. Before any analysis was done, a transformation was applied to the data to ensure that all categorical variables were expressed with binary indicators to ease the analysis. The response variable is a binary attribute called class that indicates whether the patient has a positive or negative risk for diabetes. The remaining attributes describe the patient and if they experience common symptoms related to the disease, such as weakness, itching, and obesity. A full list of the attributes and their meanings are outlined in @fig-desc in the Supplementary Materials section.

There are no missing values in this data set since missing data was already addressed by Islam et al. after data collection [-@diabetesStudy]. To verify this, we performed a check for nulls and blanks as summarized in @fig-missing in the Supplementary Materials. 

A correlation plot was created to check if there are any strong correlations between the attributes. We define a strong correlation as those with a correlation coefficient of 0.7 or larger. Based on @fig-corr, all values are lower than 0.7 so there is no evidence of strong correlations.

Next, each of the individual attributes were explored. @fig-boxplot shows a box plot of patient ages. The median age in the population is `r as.integer(unlist(ggplot_build(bplot)$data)["middle"])`. There are a few outliers that exist outside of the interquartile range. These values will be capped at `r bplot_a1`, the upper bound of the range.

@fig-barcharts shows a bar chart for each of the 16 categorical variables in the data set. There appears to be a somewhat even split between the predictor values for polyuria and itching, while the remaining predictors are not evenly split. This is especially important for the class variable because there are about 200 observations with a negative diabetes risk and about 300 observations with a positive risk. This suggests that if the model does not perform well, a resampling method such as cross-validation or bootstrapping may help improve the model.

## Timelines
This project will include two main components, a presentation and a written report. The presentation will occur on November 30th, 2023. Slides for the presentation will be completed by November 21, 2023. The written report will be finalized by December 11, 2023.


\newpage
## Supplementary Materials
### Figures

```{r fig-desc, ref.label="exp-desc", output=TRUE, message=FALSE, fig.cap="Description of attributes"}
```

```{r fig-missing, ref.label="missing", output=TRUE, message=FALSE, fig.cap="No missing data"}
```

```{r fig-corr, ref.label="corr", output=TRUE, message=FALSE, fig.cap="Correlation plot", fig.height=6, fig.width=6}
```

```{r fig-boxplot, ref.label="boxplot", output=TRUE, message=FALSE, fig.cap="Boxplot of age", fig.height=2.5, fig.width=3}
```


::: {#fig-barcharts layout-ncol=4}

```{r fig-barchart-1, ref.label="barchart-1", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-2, ref.label="barchart-2", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-3, ref.label="barchart-3", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-4, ref.label="barchart-4", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-5, ref.label="barchart-5", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-6, ref.label="barchart-6", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-7, ref.label="barchart-7", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-8, ref.label="barchart-8", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-9, ref.label="barchart-9", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-10, ref.label="barchart-10", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-11, ref.label="barchart-11", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-12, ref.label="barchart-12", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-13, ref.label="barchart-13", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-14, ref.label="barchart-14", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-15, ref.label="barchart-15", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-16, ref.label="barchart-16", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

Bar charts of categorical variables
:::

### Code

```{r code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, size=11}
```



\newpage

## References

::: {#refs}
:::

