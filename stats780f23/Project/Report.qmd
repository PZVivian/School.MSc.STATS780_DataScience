---
title: Predicting the risk of diabetes
subtitle: "STATS/CSE 780 Course Project"
author: "Pao Zhu Vivian Hsu (Student Number: 400547994)"
date: "2023-12-12"
format: pdf
editor: visual
execute:
  echo: false
  warning: false
  error: false
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
bibliography: Report.bib
csl: https://www.zotero.org/styles/apa-single-spaced
nocite: |
  @citeR
fontsize: 11pt
geometry: 
  - margin = 1in
linestretch: 1.5
---

## Top of code

```{r setup}
library(knitr)
library(tidyverse)
library(corrplot)
library(tree)
library(randomForest)
library(e1071)

```

```{r cleanse}
# ----- DATA CLEANSING ----- #
diabetes_raw <- read.csv("diabetes_data.csv", sep=";")
diabetes_int <- diabetes_raw %>% 
  mutate(gender = as.integer(ifelse(gender=="Male",1,
                            ifelse(gender=="Female",0,
                            NA))))
diabetes_pretransform <- diabetes_raw %>% 
  mutate(gender = as.factor(ifelse(gender=="Male",1,
                            ifelse(gender=="Female",0,
                            NA))),
         polyuria = as.factor(polyuria),
         polydipsia = as.factor(polydipsia),
         sudden_weight_loss = as.factor(sudden_weight_loss),
         weakness = as.factor(weakness),
         polyphagia = as.factor(polyphagia),
         genital_thrush = as.factor(genital_thrush),
         visual_blurring = as.factor(visual_blurring),
         itching = as.factor(itching),
         irritability = as.factor(irritability),
         delayed_healing = as.factor(delayed_healing),
         partial_paresis = as.factor(partial_paresis),
         muscle_stiffness = as.factor(muscle_stiffness),
         alopecia = as.factor(alopecia),
         obesity = as.factor(obesity),
         class = as.factor(class)) 

```

```{r data-desc, output=FALSE}
# ----- DATA EXPLORATION ----- #
# Data description
attribute <- c("Age","Gender","Polyuria","Polydipsia",
               "Sudden weight loss","Weakness","Polyphagia",
               "Genital thrush","Visual blurring","Itching",
               "Irritability","Delayed healing","Partial paresis",
               "Muscle stiffness","Alopecia","Obesity","Class")
values <- c("In years","1 = Male, 0 = Female","1 = Yes, 0 = No","1 = Yes, 0 = No",
            "1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No",
            "1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No",
            "1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No","1 = Yes, 0 = No",
            "1 = Positive risk, 0 = Negative risk")
data_summary <- data.frame(Attribute=attribute, Values=values)
kable(data_summary)

```

```{r missing, output=FALSE}
# Check for missing data
nulls <- sapply(diabetes_pretransform, 
                function(col){ifelse(is.na(sum(col == "")), 0, sum(col == ""))})
blanks <- sapply(diabetes_pretransform, 
                 function(col){ifelse(is.na(sum(col == "")), 0, sum(col == ""))})
kable(data.frame(Nulls=sum(nulls), Blanks=sum(blanks)))

```

```{r boxplot, output=FALSE}
# Boxplot of continuous variable
bplot <- ggplot(diabetes_pretransform, aes(y = age)) + 
  geom_boxplot() + labs(x="", y="Age") + 
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
bplot_a1 <- as.integer(unlist(ggplot_build(bplot)$data)["ymax"])
bplot + geom_hline(yintercept = bplot_a1, linetype="dotted", color="red") + 
  geom_text(aes(0.4,bplot_a1,label = bplot_a1, vjust = 1.5, color="red"), 
            show.legend = FALSE)

# Cap outliers using upper adjacent value
diabetes <- diabetes_pretransform %>% mutate(age = ifelse(age > bplot_a1, bplot_a1, age))

```

```{r barchart-1, output=FALSE}
# Bar charts for each categorical variable
ggplot(diabetes_pretransform, aes(x = gender)) + geom_bar() + 
  labs(y = "Count", x = "Gender")
```

```{r barchart-2, output=FALSE}
ggplot(diabetes_pretransform, aes(x = polyuria)) + geom_bar() + 
  labs(y = "Count", x = "Polyuria")
```

```{r barchart-3, output=FALSE}
ggplot(diabetes_pretransform, aes(x = polydipsia)) + geom_bar() + 
  labs(y = "Count", x = "Polydipsia")
```

```{r barchart-4, output=FALSE}
ggplot(diabetes_pretransform, aes(x = sudden_weight_loss)) + geom_bar() + 
  labs(y = "Count", x = "Sudden weight loss")
```

```{r barchart-5, output=FALSE}
ggplot(diabetes_pretransform, aes(x = weakness)) + geom_bar() + 
  labs(y = "Count", x = "Weakness")
```

```{r barchart-6, output=FALSE}
ggplot(diabetes_pretransform, aes(x = polyphagia)) + geom_bar() + 
  labs(y = "Count", x = "Polyphagia")
```

```{r barchart-7, output=FALSE}
ggplot(diabetes_pretransform, aes(x = genital_thrush)) + geom_bar() + 
  labs(y = "Count", x = "Genital thrush")
```

```{r barchart-8, output=FALSE}
ggplot(diabetes_pretransform, aes(x = visual_blurring)) + geom_bar() + 
  labs(y = "Count", x = "Visual blurring")
```

```{r barchart-9, output=FALSE}
ggplot(diabetes_pretransform, aes(x = itching)) + geom_bar() + 
  labs(y = "Count", x = "Itching")
```

```{r barchart-10, output=FALSE}
ggplot(diabetes_pretransform, aes(x = irritability)) + geom_bar() + 
  labs(y = "Count", x = "Irritability")
```

```{r barchart-11, output=FALSE}
ggplot(diabetes_pretransform, aes(x = delayed_healing)) + geom_bar() + 
  labs(y = "Count", x = "Delayed healing")
```

```{r barchart-12, output=FALSE}
ggplot(diabetes_pretransform, aes(x = partial_paresis)) + geom_bar() + 
  labs(y = "Count", x = "Partial paresis")
```

```{r barchart-13, output=FALSE}
ggplot(diabetes_pretransform, aes(x = muscle_stiffness)) + geom_bar() + 
  labs(y = "Count", x = "Muscle stiffness")
```

```{r barchart-14, output=FALSE}
ggplot(diabetes_pretransform, aes(x = alopecia)) + geom_bar() + 
  labs(y = "Count", x = "Alopecia")
```

```{r barchart-15, output=FALSE}
ggplot(diabetes_pretransform, aes(x = obesity)) + geom_bar() + 
  labs(y = "Count", x = "Obesity")
```

```{r barchart-16, output=FALSE}
ggplot(diabetes_pretransform, aes(x = class)) + geom_bar() + 
  labs(y = "Count", x = "Class")

```

```{r corr, output=FALSE}
# Correlation plot
corr_matrix <- cor(diabetes_int)
corrplot(round(corr_matrix,2), method = "number", number.cex=0.75)

```

```{r data-split, output=FALSE}
# ----- DATA SPLITTING ----- #
# Split the data into test and training set
set.seed(2023)
trainIndex <- sample(1:nrow(diabetes), round(nrow(diabetes)/2, 0), replace = FALSE)
diabetesTrain <- diabetes[trainIndex, ]
diabetesTest <- diabetes[-trainIndex, ]

# Pull out classes for testing
diabetesTest_class <- diabetes$class[-trainIndex]

```

### Decision tree
```{r tree-fit1, output=FALSE}
# ----- DECISION TREE ----- #
# Fit the classification tree
dTree <- tree(
  class ~ ., 
  data = diabetes, 
  subset = trainIndex,
  split = "deviance")
plot(dTree)
text(dTree, pretty = 0)

```

```{r tree-fit1-pred, output=FALSE}
# Use the fitted tree to predict results for the test data
dTree_pred <- predict(dTree, diabetesTest, type = "class")

# Compute model performance parameters
dTree_cmat <- table(dTree_pred, diabetesTest_class)
dTree_accuracy <- (dTree_cmat[1,1] + dTree_cmat[2,2])/sum(dTree_cmat)
dTree_sensitivity <- dTree_cmat[2,2]/sum(dTree_cmat[2,])
dTree_specificity <- dTree_cmat[1,1]/sum(dTree_cmat[1,])
dTree_misclass <- 1-dTree_accuracy

```

```{r tree-fit1-cv, output=FALSE}
# Get misclassification rate of different tree sizes to use for pruning
set.seed(2023)
dTreeCV <- cv.tree(dTree, FUN = prune.misclass)

# Find tree size that produces lowest misclassification rate
best_dev <- min(dTreeCV$dev)
best_size <- dTreeCV$size[dTreeCV$dev == best_dev] 

# Plot the cross-validation on a chart
plot(dTreeCV$size, dTreeCV$dev, type = "b", xlab="size", ylab="deviance") 

```

```{r tree-fit2-prune, output=FALSE}
# Prune the tree using the size with the lowest misclassification rate
dPruneTree <- prune.misclass(dTree, best = best_size)
plot(dPruneTree)
text(dPruneTree, pretty = 0)

```

```{r tree-fit2-pred, output=FALSE}
# Use the fitted tree to predict results for the test data
dPruneTree_pred <- predict(dPruneTree, diabetesTest, type = "class")

# Compute model performance parameters
dPruneTree_cmat <- table(dPruneTree_pred, diabetesTest_class)
dPruneTree_accuracy <- (dPruneTree_cmat[1,1] + dPruneTree_cmat[2,2])/
                        sum(dPruneTree_cmat)
dPruneTree_sensitivity <- dPruneTree_cmat[2,2]/sum(dPruneTree_cmat[2,])
dPruneTree_specificity <- dPruneTree_cmat[1,1]/sum(dPruneTree_cmat[1,])
dPruneTree_misclass <- 1-dPruneTree_accuracy

```

```{r tree-rforest, output=FALSE}
# Cross-validation to pick the best mtry for random forest
set.seed(2023)
cv_mtry <- c(2:10)
cv_misclass <- c()
cv_forest <- c()
for (m in cv_mtry) {
  cvForest <- randomForest(
    class ~ ., data = diabetes,
    subset = trainIndex,
    ntree = 1000,
    mtry = m)
  cvForest_pred <- predict(cvForest, newdata = diabetesTest)
  cvForest_cmat <- table(cvForest_pred, diabetesTest_class)
  cv_misclass <- c(cv_misclass, 
                   (cvForest_cmat[1,2] + cvForest_cmat[2,1])/sum(cvForest_cmat))
  cv_forest <- c(cv_forest, cvForest)
}
cv_mtry_best <- cv_mtry[cv_misclass==min(cv_misclass)][1]

# Grow random forest using best mtry
dForest <- randomForest(
  class ~ ., data = diabetes,
  subset = trainIndex,
  ntree = 1000,
  mtry = cv_mtry_best, 
  importance = TRUE)

# Use the random forest to predict results for the test data
dForest_pred <- predict(dForest, newdata = diabetesTest)

# Compute model performance parameters
dForest_cmat <- table(dForest_pred, diabetesTest_class)
dForest_accuracy <- (dForest_cmat[1,1] + dForest_cmat[2,2])/sum(dForest_cmat)
dForest_sensitivity <- dForest_cmat[2,2]/sum(dForest_cmat[2,])
dForest_specificity <- dForest_cmat[1,1]/sum(dForest_cmat[1,])
dForest_misclass <- 1-dForest_accuracy

```

```{r tree-rforest-cv, output=FALSE}
# Plot the cross-validation on a chart
plot(cv_mtry, cv_misclass, type = "b", xlab="predictors", ylab="error")

```

```{r tree-rforest-imp, output=FALSE}
# Importance of the variables
varImpPlot(dForest, main="")

```

```{r tree-summary, output=FALSE}
# Summary table of stats
as.percent <- function(value){paste0(round(value*100,2),"%")}
model <- c("Initial tree, 16 nodes", "Pruned tree, 12 nodes", "Random forest")
accuracy <- c(dTree_accuracy, dPruneTree_accuracy, dForest_accuracy)
sensitivity <- c(dTree_sensitivity, dPruneTree_sensitivity, dForest_sensitivity)
specificity <- c(dTree_specificity, dPruneTree_specificity, dForest_specificity)
misclass <- c(dTree_misclass, dPruneTree_misclass, dForest_misclass)
tree_summary <- data.frame("Model" = model, 
                           "Accuracy" = as.percent(accuracy),
                           "Sensitivity" = as.percent(sensitivity), 
                           "Specificity" = as.percent(specificity),
                           "Misclassification Rate" = as.percent(misclass),
                           check.names = FALSE)
kable(tree_summary)
```

### SVM
```{r svm-preprocess, output=FALSE}
# ----- SUPPORT VECTOR MACHINE ----- #
# Scale the data so the values are from 0 to 1
normalize0to1 <- function(data){
  (data-min(data))/(max(data)-min(data))
}
diabetesTrain_Scaled <- diabetesTrain
diabetesTest_Scaled <- diabetesTest
diabetesTrain_Scaled$age <- normalize0to1(diabetesTrain$age)
diabetesTest_Scaled$age <- normalize0to1(diabetesTest$age)

```

```{r svm-linear-cv, output=FALSE}
# Use a linear kernel and perform cross validation to find the best cost
set.seed(2023)
tune_linear <- tune(
  svm, 
  class ~ ., 
  data = diabetesTrain_Scaled, 
  kernel = "linear",
  ranges = list(cost = c(0.01, 0.05, 1, 3, 5, 10, 50, 60, 80, 100)))

plot(tune_linear, main = "")
bestmod_linear <- tune_linear$best.model

```

```{r svm-linear-pred, output=FALSE}
# Use the linear svm to predict results for the test data
linear_pred <- predict(bestmod_linear, diabetesTest_Scaled)

# Compute model performance parameters
linear_cmat <- table(linear_pred, diabetesTest_Scaled$class)
linear_accuracy <- (linear_cmat[1,1] + linear_cmat[2,2])/sum(linear_cmat)
linear_sensitivity <- linear_cmat[2,2]/sum(linear_cmat[2,])
linear_specificity <- linear_cmat[1,1]/sum(linear_cmat[1,])
linear_misclass <- 1-linear_accuracy

```

```{r svm-poly-cv, output=FALSE}
# Use a polynomial kernel and perform cross validation to find the best cost
set.seed(2023)
tune_poly <- tune(
  svm, 
  class ~ ., 
  data = diabetesTrain_Scaled, 
  kernel = "polynomial",
  ranges = list(cost = c(0.01, 0.05, 1, 10, 50, 100, 150, 200, 250, 300)))

plot(tune_poly, main = "")
bestmod_poly <- tune_poly$best.model

```

```{r svm-poly-pred, output=FALSE}
# Use the polynomial svm to predict results for the test data
poly_pred <- predict(bestmod_poly, diabetesTest_Scaled)

# Compute model performance parameters
poly_cmat <- table(poly_pred, diabetesTest_Scaled$class)
poly_accuracy <- (poly_cmat[1,1] + poly_cmat[2,2])/sum(poly_cmat)
poly_sensitivity <- poly_cmat[2,2]/sum(poly_cmat[2,])
poly_specificity <- poly_cmat[1,1]/sum(poly_cmat[1,])
poly_misclass <- 1-poly_accuracy

```

```{r svm-radial-cv, output=FALSE}
# Use a radial kernel and perform cross validation to find the best cost
set.seed(2023)
tune_radial <- tune(
  svm, 
  class ~ ., 
  data = diabetesTrain_Scaled, 
  kernel = "radial",
  ranges = list(cost = c(0.01, 0.05, 1, 10, 50, 100, 150)))

plot(tune_radial, main = "")
bestmod_radial <- tune_radial$best.model

```

```{r svm-radial-pred, output=FALSE}
# Use the radial svm to predict results for the test data
radial_pred <- predict(bestmod_radial, diabetesTest_Scaled)

# Compute model performance parameters
radial_cmat <- table(radial_pred, diabetesTest_Scaled$class)
radial_accuracy <- (radial_cmat[1,1] + radial_cmat[2,2])/sum(radial_cmat)
radial_sensitivity <- radial_cmat[2,2]/sum(radial_cmat[2,])
radial_specificity <- radial_cmat[1,1]/sum(radial_cmat[1,])
radial_misclass <- 1-radial_accuracy

```

```{r svm-summary, output=FALSE}
# Summary table of stats
model <- c("Linear", "Polynomial", "Radial")
accuracy <- c(linear_accuracy, poly_accuracy, radial_accuracy)
sensitivity <- c(linear_sensitivity, poly_sensitivity, radial_sensitivity)
specificity <- c(linear_specificity, poly_specificity, radial_specificity)
misclass <- c(linear_misclass, poly_misclass, radial_misclass)
svm_summary <- data.frame("Model" = model, 
                          "Accuracy" = as.percent(accuracy),
                          "Sensitivity" = as.percent(sensitivity), 
                          "Specificity" = as.percent(specificity),
                          "Misclassification Rate" = as.percent(misclass),
                          check.names = FALSE)
kable(svm_summary)
```

```{r final-summary, output=FALSE}
final_summary <- rbind(tree_summary, svm_summary)
model <- c("Random forest", "SVM with RBF kernel")
accuracy <- c(dForest_accuracy, radial_accuracy)
sensitivity <- c(dForest_sensitivity, radial_sensitivity)
specificity <- c(dForest_specificity, radial_specificity)
misclass <- c(dForest_misclass, radial_misclass)
final_summary <- data.frame("Model" = model, 
                            "Accuracy" = as.percent(accuracy),
                            "Sensitivity" = as.percent(sensitivity), 
                            "Specificity" = as.percent(specificity),
                            "Misclassification Rate" = as.percent(misclass),
                            check.names = FALSE)
kable(final_summary)
```


\newpage

## Abstract

## Introduction

Diabetes is a chronic disease that occurs when the body cannot effectively produce or use insulin to regulate sugar levels in the blood. According to the World Health Organization, 422 million people have diabetes worldwide and 1.5 million deaths that occur every year are directly linked to the disease [-@factsWHO]. Due to its large impact, finding a way to accurately predict diabetes has become paramount to improve global health. In this paper, we aim to address this problem by leveraging machine learning techniques to predict the risk of diabetes.

Various machine learning techniques have already been studied in current literature to find an accurate model to predict the disease [@litReview1; @litReview2]. One notable example is Islam et al.'s study which analyzes symptoms from patients of a diabetes hospital in Sylhet, Bangladesh [-@diabetesStudy]. The authors used Naive Bayes, logistic regression, and random forest methods, and found that their random forest model had the greatest accuracy among the three methods used [@diabetesStudy].

In our study, we perform a similar analysis using decision tree and support vector machine (SVM) methods. The data we have used was downloaded from an open source website called Kaggle [@diabetesData] and is the same data in Islam et al.'s study [-@diabetesStudy]. This allows us to compare our results with theirs in extension of their research.

## Methods

To begin the study, we performed data visualization and handled any detected problems by applying data transformation. We then randomly split the data into two equal parts to use as training and testing sets for both of the methods.

The first method we used was a decision tree. This method was selected because Islam et al.'s study found that a decision tree model was most accurate [-@diabetesStudy], and so our goal is to validate and reproduce the results of their tree. The first tree we fit had a terminal size of `r summary(dTree)$size` nodes. To improve the accuracy and reduce the cost complexity of the tree, we used cross-validation to select the node size that produced the lowest misclassification error rate and pruned the tree accordingly. The cross-validation is illustrated in @fig-tree-fit1-cv below. After pruning the tree, we then used random forest ensembling to further improve model performance. `r dForest$ntree` trees were grown and `r dForest$mtry` predictors were sampled for splitting at each node. We chose an arbitrary large value for the number of trees and used cross-validation to select the number of predictors to split at each node. @fig-tree-rforest-cv shows the random forest cross-validation.

::: {layout="[48,-2,48]"}
```{r fig-tree-fit1-cv, ref.label="tree-fit1-cv", output=TRUE, message=FALSE, fig.cap="Cross-validation of decision tree prior to pruning; Optimal size = 12", fig.width=5, fig.height=3.5}
```
```{r fig-tree-rforest-cv, ref.label="tree-rforest-cv", output=TRUE, message=FALSE, fig.cap="Cross-validation of random forest; Optimal number of predictors = 5", fig.width=5, fig.height=3.5}
```
:::

The second method we used was SVM. This method was selected because Islam et al. [-@diabetesStudy] have not used this method in their analyses, and so our goal is to determine whether an SVM model would better predict the risk of diabetes compared to a tree-based model. In addition, SVMs are considered as a strong model choice for binary classification [@introToStatsLearning], which is the type of data problem we are working with. We first started by scaling the data to ensure that units are between 0 and 1 across all variables. Since majority of the variables are categorical and one-hot encoded, we only needed to scale the age variable. We then performed SVM multiple times using kernel adjustments and cross-validation for cost. The three kernel types we investigated include linear, polynomial, and radial basis function (RBF) kernels. For each of these kernels, we used cross-validation to determine the cost that would produce the lowest misclassification error rate. As illustrated in @fig-svm-linear-cv, @fig-svm-poly-cv, and @fig-svm-radial-cv, the optimal costs were `r bestmod_linear$cost`, `r bestmod_poly$cost`, and `r bestmod_radial$cost` for the linear, polynomial, and RBF kernels respectively.

After building each model, we computed the accuracy, specificity, sensitivity, and misclassification error rate to assess model performance. These performance parameters were selected because they are often used for binary classification problems. Moreover, a few of these metrics were used by Islam et al. [-@diabetesStudy], so computing them facilitated direct comparison with their models. A comparison of these four measurements between models helped us determine which model is a stronger fit to predict diabetes.

::: {layout="[[48,-2,48], [-20,60,-20]]"}
```{r fig-svm-linear-cv, ref.label="svm-linear-cv", output=TRUE, message=FALSE, fig.cap="Cross-validation for linear SVM; Optimal cost = 3", fig.width=5}
```
```{r fig-svm-poly-cv, ref.label="svm-poly-cv", output=TRUE, message=FALSE, fig.cap=" Cross-validation for polynomial SVM; Optimal cost = 150", fig.width=5}
```
```{r fig-svm-radial-cv, ref.label="svm-radial-cv", output=TRUE, message=FALSE, fig.cap="Cross-validation for RBF SVM; Optimal cost = 50", fig.width=5}
```
:::

## Results

### Exploratory Data Analysis

The data set consists of `r nrow(diabetes)` observations and `r ncol(diabetes)` attributes. Before any analysis was done, a transformation was applied to the data to ensure that all categorical variables were expressed with binary indicators to ease the analysis. The response variable is a binary attribute called class that indicates whether the patient has a positive or negative risk for diabetes. The remaining attributes describe the patient and if they experience common symptoms related to the disease, such as weakness, itching, and obesity. A full list of the attributes and their meanings are outlined in Supp. @tbl-data-desc.

There are no missing values in this data set since missing data was already addressed by Islam et al. after data collection [-@diabetesStudy]. To verify this, we performed a check for nulls and blanks as summarized in Supp. @tbl-missing.

A correlation plot was created to check if there are any strong correlations between the attributes. We define a strong correlation as those with a correlation coefficient of 0.7 or larger. Based on @fig-corr, all values are lower than 0.7 so there is no evidence of strong correlations.

```{r fig-corr, ref.label="corr", output=TRUE, message=FALSE, fig.cap="Correlation plot", fig.height=6, fig.width=6}
```

Next, each of the individual attributes were explored. @fig-boxplot below shows a box plot of patient ages. The median age in the population is `r as.integer(unlist(ggplot_build(bplot)$data)["middle"])`. There are a few outliers that exist outside of the interquartile range. These values were capped at `r bplot_a1`, the upper bound of the range.

```{r fig-boxplot, ref.label="boxplot", output=TRUE, message=FALSE, fig.cap="Boxplot of age", fig.height=2.5, fig.width=3}
```

We also plotted the 16 categorical variables in bar charts as illustrated in Supp. @fig-barcharts. There appears to be a somewhat even split between the predictor values for polyuria and itching, while the remaining predictors are not evenly split. This is especially important for the class variable because there are about 200 observations with a negative diabetes risk and about 300 observations with a positive risk. This suggests that if the model does not perform well, a resampling method such as cross-validation or bootstrapping may help improve the model. As such, we have first modelled the data in its original distribution and allowed for adjustments as required.

### Decision Tree

@tbl-tree-summary summarizes the performance of the tree-based models. The initial tree had an accuracy of `r as.percent(dTree_accuracy)`. Its ability to predict those with diabetes risk is about `r as.percent(dTree_sensitivity - dTree_specificity)` higher than its ability to predict those without diabetes risk.

```{r}
#| label: tbl-tree-summary
#| ref-label: tree-summary
#| tbl-cap: Model performance parameters for tree-based methods
#| output: TRUE
#| message: FALSE
```

While pruning the tree reduced its cost complexity, no improvements to the accuracy were observed. Nonetheless, the pruned tree gives us insight into what variables and values are linked to a greater risk for diabetes as shown in @fig-tree-fit2-prune. In particular, we can see that polydipsia and gender are the first two variables to appear on the tree and are likely strong predictors for the disease. Additionally, the first internal node of the tree shows that the presence of polydipsia is likely linked with diabetes risk while the absence of polydipsia requires additional variables to determine risk.

```{r fig-tree-fit2-prune, ref.label="tree-fit2-prune", output=TRUE, message=FALSE, fig.cap="Pruned decision tree with 12 terminal nodes", fig.height=5, fig.width=7}
```

The random forest had the best performance among the three models with an accuracy of `r as.percent(dForest_accuracy)`. Its ability to predict cases of diabetes risk and cases without diabetes risk is nearly even. From the random forest, we were also able to obtain a more thorough analysis on variable importance. In particular, the most important variables in predicting diabetes include polyuria, polydipsia, gender, and age as shown in @fig-tree-rforest-imp. This is determined using both the mean decrease in accuracy and Gini index. Being aware of these four variables may help healthcare professionals identify patients with a greater risk for the disease and facilitate early diagnosis.

```{r fig-tree-rforest-imp, ref.label="tree-rforest-imp", output=TRUE, message=FALSE, fig.cap="Importance of variables from random forest; Top is the most important and bottom is the least important", fig.height=5, fig.width=8}
```

It is important to note that there is a moderate correlation between polyuria and polydipsia as observed in @fig-corr which we've presented earlier. To clarify, polyuria is a medical condition characterized by excessive urination while polydipsia is characterized by excessive water consumption [@polyuria; @polydipsia]. Since water consumption leads to urination, this could explain why both variables are similar in terms of importance. While we consider this correlation to be a moderate one, future studies could remove one of the variables or increase the random forest tree size if they believe there is a large variance resulting from their correlation.


### Support Vector Machine

@tbl-svm-summary summarizes the performance of the SVM models.

```{r}
#| label: tbl-svm-summary
#| ref-label: svm-summary
#| tbl-cap: Model performance parameters for SVM methods
#| output: TRUE
#| message: FALSE
```

## Conclusion
```{r}
#| label: tbl-final-summary
#| ref-label: final-summary
#| tbl-cap: Comparison of performance parameters between the best tree-based and SVM model
#| output: TRUE
#| message: FALSE
```

- Increasing random forest size
- Computational cost (of SVM)
- Kernel adjustments
- Generalizability (only in Bangladesh)

\newpage

## Supplementary Materials

### Tables and Figures

```{r}
#| label: tbl-data-desc
#| ref-label: data-desc
#| tbl-cap: Description of attributes
#| output: TRUE
#| message: FALSE
#| tbl-height: 3
#| tbl-width: 3
```

```{r}
#| label: tbl-missing
#| ref-label: missing
#| tbl-cap: No missing data
#| output: TRUE
#| message: FALSE
#| tbl-height: 3
#| tbl-width: 3
```

::: {#fig-barcharts layout-ncol="4"}
```{r fig-barchart-1, ref.label="barchart-1", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-2, ref.label="barchart-2", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-3, ref.label="barchart-3", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-4, ref.label="barchart-4", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-5, ref.label="barchart-5", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-6, ref.label="barchart-6", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-7, ref.label="barchart-7", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-8, ref.label="barchart-8", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-9, ref.label="barchart-9", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-10, ref.label="barchart-10", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-11, ref.label="barchart-11", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-12, ref.label="barchart-12", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-13, ref.label="barchart-13", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-14, ref.label="barchart-14", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-15, ref.label="barchart-15", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

```{r fig-barchart-16, ref.label="barchart-16", output=TRUE, message=FALSE, fig.height=1.4, fig.width=1.5}
```

Bar charts of categorical variables
:::

### Code

```{r code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, size=11}
```

\newpage

## References

::: {#refs}
:::
