---
title: |
  | STATS/CSE 780
  | Assignment 2
author: "Pao Zhu Vivian Hsu (Student Number: 400547994)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: pdf
editor: visual
execute:
  echo: false
  warning: false
  error: false
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
bibliography: A2.bib
csl: https://www.zotero.org/styles/apa-single-spaced
nocite: |
  @xie2020r
fontsize: 11pt
geometry: 
  - margin = 1in
linestretch: 1.5
---

\newpage

```{r setup, output=FALSE}
# ----- LOAD PACKAGES AND DATA ----- #
packages <- c("knitr", "dplyr", "tidyverse", "DescTools", "ggplot2", "corrplot",
              "class", "pROC", "glmnet", "caret")
lapply(packages, library, character.only = TRUE)
bank_raw <- read.csv("Churn_Modelling.csv")


```

```{r exploration-1, output=FALSE}
# ----- DATA EXPLORATION ----- #
# Check data types, min, max, and missing data
data_type <- sapply(bank_raw,class)
min <- sapply(bank_raw, function(col){min(col,na.rm=TRUE)})
max <- sapply(bank_raw, function(col){max(col,na.rm=TRUE)})
nulls <- sapply(bank_raw, function(col){sum(is.na(col))})
blanks <- sapply(bank_raw, 
                 function(col){ifelse(is.na(sum(col == "")), 0, sum(col == ""))})
data_summary <- data.frame(row.names = names(nulls), data_type=data_type, 
                           min=min, max=max, nulls=nulls, blanks=blanks)
kable(data_summary)

```
```{r exploration-2, output=FALSE}
# Check dimensions and reason for numeric Age
dim(bank_raw) # There are duplicate rows
bank_raw$Age[round(bank_raw$Age) != bank_raw$Age] # There are decimals

```

```{r cleansing}
# ----- CLEANSE DATA ----- #
bank <- bank_raw %>%
  
  # Remove columns that are not important for analysis
  dplyr::select(-c("RowNumber", "CustomerId", "Surname")) %>%
  
  # Impute missing values with mean, median, and mode
  mutate(Age = replace_na(Age, round(mean(Age,na.rm=TRUE),0)),
         HasCrCard = replace_na(HasCrCard, median(HasCrCard,na.rm=TRUE)),
         IsActiveMember = replace_na(IsActiveMember, median(HasCrCard,na.rm=TRUE)),
         Geography = ifelse(Geography == "", Mode(Geography,na.rm=TRUE)[1], Geography)) %>% 
  
  # Change data types
  mutate(Age = as.integer(round(Age)),
         Geography = as.factor(unclass(as.factor(Geography))),
         Gender = as.factor(unclass(as.factor(Gender))),
         HasCrCard = as.factor(HasCrCard),
         IsActiveMember = as.factor(IsActiveMember),
         Exited = as.factor(Exited))

# Remove duplicated rows
bank <- bank[!duplicated(bank), ]

```

```{r boxplot-1, output=FALSE}
# ----- DATA VISUALIZATION  ----- #
# Create a boxplot for each continuous variable 
ggplot(bank, aes(x = Exited, y = CreditScore)) + geom_boxplot() + ylab("Credit Score")
```
```{r boxplot-2, output=FALSE}
ggplot(bank, aes(x = Exited, y = Age)) + geom_boxplot()
```
```{r boxplot-3, output=FALSE}
ggplot(bank, aes(x = Exited, y = Tenure)) + geom_boxplot()
```
```{r boxplot-4, output=FALSE}
ggplot(bank, aes(x = Exited, y = Balance)) + geom_boxplot()
```
```{r boxplot-5, output=FALSE}
ggplot(bank, aes(x = Exited, y = NumOfProducts)) + geom_boxplot() + 
  ylab("Number of Products")
```
```{r boxplot-6, output=FALSE}
ggplot(bank, aes(x = Exited, y = EstimatedSalary)) + geom_boxplot() + 
  ylab("Estimated Salary")

```

```{r barchart-1, output=FALSE}
# Create a bar chart for each categorical variable
ggplot(bank, aes(x = HasCrCard, fill = Exited)) + geom_bar(position = "dodge") +
  labs(y = "Count", x = "Has Credit Card")
```
```{r barchart-2, output=FALSE}
ggplot(bank, aes(x = IsActiveMember, fill = Exited)) + geom_bar(position = "dodge") + 
  labs(y = "Count", x = "Is Active Member")
```
```{r barchart-3, output=FALSE}
ggplot(bank, aes(x = Geography, fill = Exited)) + geom_bar(position = "dodge") + 
  labs(y = "Count")
```
```{r barchart-4, output=FALSE}
ggplot(bank, aes(x = Gender, fill = Exited)) + geom_bar(position = "dodge") + 
  labs(y = "Count")


```

```{r corrplot, output=FALSE}
# Check for correlation between continuous variables
corr_matrix <- cor(bank %>% dplyr::select(-Geography, -Gender, -HasCrCard, 
                                          -IsActiveMember, -Exited))
corrplot(round(corr_matrix,2), method = "number")


```

```{r split}
# ----- SPLIT INTO TRAIN & TEST DATA ----- #
set.seed(2023780)

train_index <- sample(1:nrow(bank), round(nrow(bank)/2, 0), replace = FALSE)

# Training set
train_data <- bank[train_index, ]
train_x <- dplyr::select(train_data, -Exited)
train_y <- dplyr::pull(train_data, Exited)

# Testing set
test_data <- bank[-train_index, ]
test_x <- dplyr::select(test_data, -Exited)
test_y <- dplyr::pull(test_data, Exited)


```

```{r log-reg-1, output=FALSE}
# ----- LOGISTIC REGRESSION ----- #
set.seed(2023780)

# Include all variables as predictors in the regression
log_mod0 <- glm(Exited ~ ., family = binomial("logit"), data = train_data)
summary(log_mod0)

```
```{r log-reg-2, output=FALSE}
set.seed(2023780)

# Remove predictors with p-values that are not significant (i.e. > 0.05)
log_mod1 <- update(log_mod0, ~ . -CreditScore -Tenure -NumOfProducts 
                   -HasCrCard -EstimatedSalary -Geography)
summary(log_mod1)

```
```{r log-reg-3, output=FALSE}
# Predict outcome probabilities using test set
log_mod1_y_prob <- predict(log_mod1, newdata = test_data, type = "response")

# To label the outcomes, find the optimal cut-off value using ROC curve
log_mod1_pROC <- roc(test_y, log_mod1_y_prob, smoothed = TRUE, ci=TRUE, ci.alpha=0.9, 
                     plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                     print.auc=TRUE, show.thres=TRUE)
cutoff <- coords(log_mod1_pROC, "best")$threshold

# Assign labels to prediction results using cut-off value
log_mod1_y <- ifelse(log_mod1_y_prob > cutoff, 1, 0)

```

```{r knn-class}
# ----- K-NEAREST NEIGHBOUR CLASSIFICATION ----- #
set.seed(2023780)

# Find optimal k value using k-fold cross validation and build the KNN model
knn_ctrl <- trainControl(method = "repeatedcv", number = 15, repeats = 3)
knn_mod1 <- train(Exited ~ ., data = train_data, method = "knn", trControl=knn_ctrl,
                 preProcess = c("center", "scale"))

# Predict outcome using test set
knn_mod1_y <- predict(knn_mod1, newdata = test_data)


```

```{r performance, output=FALSE}
# ----- CLASSIFIER PERFORMANCE ----- #
# Miss-classification error rate: % of churn incorrectly predicted
# Accuracy: % of churn correctly predicted
# Sensitivity: % correctly predicted as churned
# Specificity: % correctly predicted as not churned

# --- LOGISTIC REGRESSION MODEL PERFORMANCE --- #
log_mod1_cmatrix <- table(log_mod1_y, test_y) # Confusion matrix
log_mod1_mcerate <- mean(log_mod1_y != test_y) # Miss-classification error rate
log_mod1_accuracy <- mean(log_mod1_y == test_y) # Accuracy
log_mod1_sensitivity <- log_mod1_cmatrix[2,2]/sum(log_mod1_cmatrix[2,]) # Sensitivity
log_mod1_specificity <- log_mod1_cmatrix[1,1]/sum(log_mod1_cmatrix[1,]) # Specificity
log_mod1_stats <- c(log_mod1_mcerate, log_mod1_accuracy, log_mod1_sensitivity, 
                    log_mod1_specificity)

# --- KNN MODEL PERFORMANCE --- #
knn_mod1_cmatrix <- table(knn_mod1_y, test_y) # Confusion matrix
knn_mod1_mcerate <- mean(knn_mod1_y != test_y) # Miss-classification error rate
knn_mod1_accuracy <- mean(knn_mod1_y == test_y) # Accuracy
knn_mod1_sensitivity <- knn_mod1_cmatrix[2,2]/sum(knn_mod1_cmatrix[2,]) # Sensitivity
knn_mod1_specificity <- knn_mod1_cmatrix[1,1]/sum(knn_mod1_cmatrix[1,]) # Specificity
knn_mod1_stats <- c(knn_mod1_mcerate, knn_mod1_accuracy, knn_mod1_sensitivity, 
                    knn_mod1_specificity)

# --- COMPARE MODELS --- #
comparison <- data.frame(row.names = c("Miss-classification error rate", "Accuracy", 
                                       "Sensitivity", "Specificity"),
                         "knn" = round(knn_mod1_stats, 2),
                         "logistic_regression" = round(log_mod1_stats, 2))
kable(comparison)

```



## Introduction

The goal of this study is to develop a model that predicts customer churn for a bank and provide suggestions that can help the bank reduce the rate of churn. Two models were built to predict customer churn; the first was built using logistic regression and the second was built using K-nearest neighbour (KNN) classification. Both models showed that X is an important predictor of churn. A comparison of accuracy rates showed that the KNN model is a stronger fit than the logistic model.

## Methods

The data for this study was downloaded from the Kaggle website [-@bankData]. It consists of `r ncol(bank_raw)` variables and `r nrow(bank_raw)` observations from a bank that operates in France, Germany, and Spain. The data set includes a binary indicator for customer churn (called "exited") and a variety of columns describing the customer, such as age, credit score, and estimated salary. The categorical churn variable makes this data suitable for logistic regression and KNN classification. 

Prior to any modelling, the data was first screened for missing data, duplicate rows, unexpected data ranges, and data type issues. These issues were addressed through data imputation and transformation. A summary of the data issues is available in @fig-stat-summary in the Supplementary Materials. Additionally, row numbers, customer ids, and surnames were removed from the data set as they are not important for studying customer churn. The remaining 10 variables were used as predictors for classification.

Box plots, bar charts, and a correlation plot were created to visualize trends in the data. Of the six box plots shown in @fig-boxplots, age, balance, and number of products appeared to have the greatest impact on churn. Those who churned tend to be older, have a greater bank balance, and have less products with the bank compared to those who did not churn. The box plots for credit score, age, and number of products also show potential outliers. These outliers were not removed since these customers do not appear to be outliers for many of the other variables. Furthermore, the values for these variables do not appear to be unreasonably large or small from a practical perspective. For the bar charts in @fig-barcharts, customers who churned appeared to be slightly less active and female rather than male compared to those who did not churn. But overall, the bar charts did not appear to show any strong patterns. Finally, the correlation plot in @fig-corr showed that there was low correlation between the continuous variables.

After visualization, a logistic regression model of binomial family with a logit link was built. Based on Harrell's 1:15 rule of thumb for the number of predictor variables compared to observations in generalized linear models [-@Harrell], all 10 variables were included in the model.

Next, KNN classification was used to create another model to predict customer churn. The final model was fit using k=`r knn_mod1$bestTune` nearest neighbours. This value of k was determined using k-fold cross validation.


## Results
```{r fig-performance, ref.label="performance", output=TRUE, message=FALSE, fig.cap="Model performance comparison"}
```


## Conclusion
- KNN model is better
- important predictors
- ways to apply results to real world
- suggestions for future models and improvement


\newpage

## Supplementary material
### Data definitions for categorical variables
Here are the data definitions for categorical variables in the final data set. Definitions for the continuous variables remain the same as the original data source (@bankData).

* Exited: Whether the customer has a churned or not (0 = not churned, 1 = churned)
* Gender: The customer's gender (1 = female, 2 = male)
* Geography: Country where the customer lives (1 = France, 2 = Germany, 3 = Spain)
* Has credit card: Whether the customer has a credit card or not (0 = no, 1 = yes)
* Is active member: Whether the customer is active in the bank or not (0 = no, 1 = yes)

### Figures
```{r fig-stat-summary, ref.label="exploration-1", output=TRUE, message=FALSE, fig.cap="Summary of the original Kaggle data used prior to cleansing", fig.height=4, fig.width=3}
```

::: {#fig-boxplots layout-ncol=2}
```{r fig-boxplot-1, ref.label="boxplot-1", output=TRUE, message=FALSE}
```
```{r fig-boxplot-2, ref.label="boxplot-2", output=TRUE, message=FALSE}
```
```{r fig-boxplot-3, ref.label="boxplot-3", output=TRUE, message=FALSE}
```
```{r fig-boxplot-4, ref.label="boxplot-4", output=TRUE, message=FALSE}
```
```{r fig-boxplot-5, ref.label="boxplot-5", output=TRUE, message=FALSE}
```
```{r fig-boxplot-6, ref.label="boxplot-6", output=TRUE, message=FALSE}
```
Boxplots of continuous variables
:::

::: {#fig-barcharts layout-ncol=2}
```{r fig-barchart-1, ref.label="barchart-1", output=TRUE, message=FALSE}
```
```{r fig-barchart-2, ref.label="barchart-2", output=TRUE, message=FALSE}
```
```{r fig-barchart-3, ref.label="barchart-3", output=TRUE, message=FALSE}
```
```{r fig-barchart-4, ref.label="barchart-4", output=TRUE, message=FALSE}
```
Bar charts of categorical variables
:::

```{r fig-corrplot, ref.label="corrplot", output=TRUE, message=FALSE, fig.cap="Correlation between continuous variables"}
```

```{r fig-reg, ref.label="log-reg-2", output=TRUE, message=FALSE, fig.cap="Final logistic regression model"}
```

```{r fig-reg, ref.label="log-reg-3", output=TRUE, message=FALSE, fig.cap="ROC curve"}
```



### Code
```{r code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, size=11}
```

\newpage

## References

::: {#refs}
:::
