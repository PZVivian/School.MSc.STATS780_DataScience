---
title: |
  | STATS/CSE 780
  | Assignment 2
author: "Pao Zhu Vivian Hsu (Student Number: 400547994)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: pdf
editor: visual
execute:
  echo: false
  warning: false
  error: false
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
bibliography: A2.bib
csl: https://www.zotero.org/styles/apa-single-spaced
nocite: |
  @xie2020r
fontsize: 11pt
geometry: 
  - margin = 1in
linestretch: 1.5
---

\newpage

```{r setup, output=FALSE}
# ----- LOAD PACKAGES AND DATA ----- #
packages <- c("knitr", "dplyr", "tidyverse", "DescTools", "ggplot2", "corrplot",
              "class", "pROC", "glmnet", "caret")
lapply(packages, library, character.only = TRUE)
bank_raw <- read.csv("Churn_Modelling.csv")

```

```{r exploration-1, output=FALSE}
# ----- DATA EXPLORATION ----- #
# Check data types, min, max, and missing data
data_type <- sapply(bank_raw,class)
min <- sapply(bank_raw, function(col){min(col,na.rm=TRUE)})
max <- sapply(bank_raw, function(col){max(col,na.rm=TRUE)})
nulls <- sapply(bank_raw, function(col){sum(is.na(col))})
blanks <- sapply(bank_raw, 
                 function(col){ifelse(is.na(sum(col == "")), 0, sum(col == ""))})
data_summary <- data.frame(row.names = names(nulls), data_type=data_type, 
                           min=min, max=max, nulls=nulls, blanks=blanks)
kable(data_summary)

```
```{r exploration-2, output=FALSE}
# Check dimensions and reason for numeric Age
dim(bank_raw) # There are duplicate rows
bank_raw$Age[round(bank_raw$Age) != bank_raw$Age] # There are decimals

```

```{r cleansing}
# ----- CLEANSE DATA ----- #
bank <- bank_raw %>%
  # Remove columns that are not important for analysis
  dplyr::select(-c("RowNumber", "CustomerId", "Surname")) %>%
  # Impute missing values with mean, median, and mode
  mutate(Age = replace_na(Age, round(mean(Age,na.rm=TRUE),0)),
         HasCrCard = replace_na(HasCrCard, median(HasCrCard,na.rm=TRUE)),
         IsActiveMember = replace_na(IsActiveMember, median(HasCrCard,na.rm=TRUE)),
         Geography = ifelse(Geography == "", Mode(Geography,na.rm=TRUE)[1], Geography)) %>% 
  # Change data types
  mutate(Age = as.integer(round(Age)),
         Geography = as.factor(unclass(as.factor(Geography))),
         Gender = as.factor(unclass(as.factor(Gender))),
         HasCrCard = as.factor(HasCrCard),
         IsActiveMember = as.factor(IsActiveMember),
         Exited = as.factor(Exited))
# Remove duplicated rows
bank <- bank[!duplicated(bank), ]

```

```{r boxplot-1, output=FALSE}
# ----- DATA VISUALIZATION  ----- #
# Create a boxplot for each continuous variable 
ggplot(bank, aes(x = Exited, y = CreditScore)) + geom_boxplot() + ylab("Credit Score")
```
```{r boxplot-2, output=FALSE}
ggplot(bank, aes(x = Exited, y = Age)) + geom_boxplot()
```
```{r boxplot-3, output=FALSE}
ggplot(bank, aes(x = Exited, y = Tenure)) + geom_boxplot()
```
```{r boxplot-4, output=FALSE}
ggplot(bank, aes(x = Exited, y = Balance)) + geom_boxplot()
```
```{r boxplot-5, output=FALSE}
ggplot(bank, aes(x = Exited, y = NumOfProducts)) + geom_boxplot() + 
  ylab("Number of Products")
```
```{r boxplot-6, output=FALSE}
ggplot(bank, aes(x = Exited, y = EstimatedSalary)) + geom_boxplot() + 
  ylab("Estimated Salary")

```

```{r barchart-1, output=FALSE}
# Create a bar chart for each categorical variable
ggplot(bank, aes(x = HasCrCard, fill = Exited)) + geom_bar(position = "dodge") +
  labs(y = "Count", x = "Has Credit Card")
```
```{r barchart-2, output=FALSE}
ggplot(bank, aes(x = IsActiveMember, fill = Exited)) + geom_bar(position = "dodge") + 
  labs(y = "Count", x = "Is Active Member")
```
```{r barchart-3, output=FALSE}
ggplot(bank, aes(x = Geography, fill = Exited)) + geom_bar(position = "dodge") + 
  labs(y = "Count")
```
```{r barchart-4, output=FALSE}
ggplot(bank, aes(x = Gender, fill = Exited)) + geom_bar(position = "dodge") + 
  labs(y = "Count")

```

```{r corrplot, output=FALSE}
# Check for correlation between continuous variables
corr_matrix <- cor(bank %>% dplyr::select(-Geography, -Gender, -HasCrCard, 
                                          -IsActiveMember, -Exited))
corrplot(round(corr_matrix,2), method = "number")

```

```{r split}
# ----- SPLIT INTO TRAIN & TEST DATA ----- #
set.seed(2023780)
train_index <- sample(1:nrow(bank), round(nrow(bank)/2, 0), replace = FALSE)

# Training set
train_data <- bank[train_index, ]
train_x <- dplyr::select(train_data, -Exited)
train_y <- dplyr::pull(train_data, Exited)

# Testing set
test_data <- bank[-train_index, ]
test_x <- dplyr::select(test_data, -Exited)
test_y <- dplyr::pull(test_data, Exited)

```

```{r log-reg-1, output=FALSE}
# ----- LOGISTIC REGRESSION ----- #
set.seed(2023780)

# Include all variables as predictors in the initial model
log_mod0 <- glm(Exited ~ ., family = binomial("logit"), data = train_data)
summary(log_mod0)

# Perform backward selection to select most significant predictors
log_mod0 <- update(log_mod0, ~ . -HasCrCard)
summary(log_mod0)
log_mod0 <- update(log_mod0, ~ . -Geography)
summary(log_mod0)
log_mod0 <- update(log_mod0, ~ . -EstimatedSalary)
summary(log_mod0)
log_mod1 <- update(log_mod0, ~ . -NumOfProducts)
summary(log_mod1)
```
```{r log-reg-2, output=FALSE}
summary <- data.frame(round(summary(log_mod1)$coefficients, 4))
colnames(summary) <- c("estimate", "std_error", "z_value", "pr(>|z|)")
kable(summary)

```
```{r log-reg-3, output=FALSE}
# Predict outcome probabilities using test set
log_mod1_y_prob <- predict(log_mod1, newdata = test_data, type = "response")

# To label the outcomes, find the optimal cut-off value using ROC curve
log_mod1_pROC <- roc(test_y, log_mod1_y_prob, smoothed = TRUE, ci=TRUE, ci.alpha=0.9, 
                     plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                     print.auc=TRUE, show.thres=TRUE)
cutoff <- coords(log_mod1_pROC, "best")$threshold

# Assign labels to prediction results using cut-off value
log_mod1_y <- ifelse(log_mod1_y_prob > cutoff, 1, 0)

```

```{r knn-class}
# ----- K-NEAREST NEIGHBOUR CLASSIFICATION ----- #
# Find optimal k value using k-fold cross validation and build the KNN model
set.seed(2023780)
knn_ctrl <- trainControl(method = "repeatedcv", number = 15, repeats = 3)
knn_mod1 <- train(Exited ~ ., data = train_data, method = "knn", trControl=knn_ctrl,
                 preProcess = c("center", "scale"))

# Predict outcome using test set
knn_mod1_y <- predict(knn_mod1, newdata = test_data)

```

```{r performance, output=FALSE}
# ----- CLASSIFIER PERFORMANCE ----- #
# --- LOGISTIC REGRESSION MODEL PERFORMANCE --- #
log_mod1_cmatrix <- table(log_mod1_y, test_y) # Confusion matrix
log_mod1_mcerate <- mean(log_mod1_y != test_y) # Miss-classification error rate
log_mod1_accuracy <- mean(log_mod1_y == test_y) # Accuracy
log_mod1_sensitivity <- log_mod1_cmatrix[2,2]/sum(log_mod1_cmatrix[2,]) # Sensitivity
log_mod1_specificity <- log_mod1_cmatrix[1,1]/sum(log_mod1_cmatrix[1,]) # Specificity
log_mod1_stats <- c(log_mod1_mcerate, log_mod1_accuracy, log_mod1_sensitivity, 
                    log_mod1_specificity)

# --- KNN MODEL PERFORMANCE --- #
knn_mod1_cmatrix <- table(knn_mod1_y, test_y) # Confusion matrix
knn_mod1_mcerate <- mean(knn_mod1_y != test_y) # Miss-classification error rate
knn_mod1_accuracy <- mean(knn_mod1_y == test_y) # Accuracy
knn_mod1_sensitivity <- knn_mod1_cmatrix[2,2]/sum(knn_mod1_cmatrix[2,]) # Sensitivity
knn_mod1_specificity <- knn_mod1_cmatrix[1,1]/sum(knn_mod1_cmatrix[1,]) # Specificity
knn_mod1_stats <- c(knn_mod1_mcerate, knn_mod1_accuracy, knn_mod1_sensitivity, 
                    knn_mod1_specificity)

# --- COMPARE MODELS --- #
comparison <- data.frame(row.names = c("Miss-classification error rate", "Accuracy", 
                                       "Sensitivity", "Specificity"),
                         "logistic_regression" = round(log_mod1_stats, 2),
                         "knn" = round(knn_mod1_stats, 2))
kable(comparison)

```

```{r fig-lasso-1, output=FALSE}
# ----- LOGISTIC REGRESSION WITH SHRINKAGE ----- #
# Format training and testing data into matrices and create lambda grid
set.seed(2023780)
x <- model.matrix(Exited ~ CreditScore + Gender + Age + Tenure + Balance + IsActiveMember,
                  bank)[, -1]
y <- dplyr::pull(bank, Exited)

# Perform cross validation to find lambda
lasso_cv <- cv.glmnet(x[train_index, ], y[train_index], family="binomial",
                      alpha = 1, standardize = FALSE)

# Predict the log of odds of being churned and convert to probabilities
lasso_test_pred_log_odds <- predict(lasso_cv, newx = x[-train_index, ], 
                               s = "lambda.1se")[,1]
lasso_test_pred_prob <- exp(lasso_test_pred_log_odds)/(1+exp(lasso_test_pred_log_odds))
df_lasso_test <- tibble(pre_prob = lasso_test_pred_prob, 
                        y = slice(bank, -train_index) %>% pull(Exited))

# Get the logistic regression model after shrinkage
lasso_mod1 <- glmnet(x[train_index, ], y[train_index], family="binomial",
              alpha = 1, standardize = FALSE)
lasso_coefs <- predict(lasso_mod1, type = "coefficients", s = lasso_cv$lambda.1se)

# Use an ROC curve to get the optimal cutoff
pROC_graph <- roc(df_lasso_test$y, df_lasso_test$pre_prob, smoothed = TRUE,
                  ci=TRUE, ci.alpha=0.9, stratified=FALSE, plot=TRUE, auc.polygon=TRUE, 
                  max.auc.polygon=TRUE, grid=TRUE, print.auc=TRUE, show.thres=TRUE)
cutoff_2 <- coords(pROC_graph, "best")$threshold

# Compute the test error
lasso_test_class <- df_lasso_test %>%
  dplyr::mutate(class_pred = ifelse(pre_prob <= cutoff_2, levels(bank$Exited)[1], 
                             levels(bank$Exited)[2]))

# Create a confusion matrix
tab <- table(pull(df_lasso_test, y), pull(lasso_test_class, class_pred))
stat <- 1-sum(diag(tab))/(sum(tab))
lasso_conf_matrix <- caret::confusionMatrix(
  factor(lasso_test_class$class_pred), lasso_test_class$y, mode = "everything", 
  positive="1")

```


## Introduction

The goal of this study is to develop a model that predicts customer churn for a bank and provide suggestions that can help the bank reduce the rate of churn. Two models were built to predict customer churn; the first was built using logistic regression and the second was built using K-nearest neighbour (KNN) classification. A comparison of accuracy rates showed that the KNN model is a stronger fit than the logistic model.

## Methods

The data for this study was downloaded from the Kaggle website [-@bankData]. It consists of `r ncol(bank_raw)` variables and `r nrow(bank_raw)` observations from a bank that operates in France, Germany, and Spain. The data set includes a binary indicator for customer churn (called "exited") and a variety of columns describing the customer, such as age, credit score, and estimated salary. The categorical churn variable makes this data suitable for logistic regression and KNN classification. 

Prior to any modelling, the data was first screened for missing data, duplicate rows, unexpected data ranges, and data type issues. These issues were addressed through data imputation and transformation. A summary of the data issues is available in @fig-stat-summary in the Supplementary Materials. Additionally, row numbers, customer ids, and surnames were removed from the data set as they are not important for studying customer churn. The remaining 10 variables were used as predictors for classification.

Box plots, bar charts, and a correlation plot were created to visualize trends in the data. Of the six box plots shown in @fig-boxplots, age, balance, and number of products appeared to have the greatest impact on churn. Those who churned tend to be older, have a greater bank balance, and have less products with the bank compared to those who did not churn. The box plots for credit score, age, and number of products also show potential outliers. These outliers were not removed since these customers do not appear to be outliers for many of the other variables. Furthermore, the values for these variables do not appear to be unreasonably large or small from a practical perspective. For the bar charts in @fig-barcharts, customers who churned appeared to be slightly less active and female rather than male compared to those who did not churn. But overall, the bar charts did not appear to show any strong patterns. Finally, the correlation plot in @fig-corrplot showed that there was low correlation between the continuous variables.

Once the data was cleaned, it was randomly split into two equal parts for training and testing. The first model that was built was a logistic regression model. Based on Harrell's 1:15 rule of thumb for a suitable number of predictor variables compared to observations [-@Harrell], all 10 variables were included in the model initially. The final model includes only 6 of these variables since 4 were removed with backward selection. A sensitivity and specificity analysis with an ROC curve was performed to choose the optimal cutoff value of `r round(cutoff,2)` for interpreting the predictions. A summary of the final model and the ROC curve can be found in @fig-reg and @fig-roc of the Supplementary Materials respectively. Next, KNN classification was used to create another model to predict customer churn. K-fold cross validation was used to determine a neighbourhood size of k=`r knn_mod1$bestTune`. 

Both models were tested using the hold-out set and statistics on the model performance were computed and compared between the models. As a result of the comparison, shrinkage methods were applied to the logistic regression model in attempt to further improve the model. Due to the model's poor accuracy of `r round(lasso_conf_matrix$overall[1],2)` in comparison with the other two models, it was not used for any further interpretation.


## Results

The most important variable in the logistic model is the customer's active status in the bank. Its regression coefficient is `r round(as.numeric(coef(log_mod1)[7]), 2)`, which suggests that active members are exp(`r round(as.numeric(coef(log_mod1)[7]), 2)`)=`r round((1-exp(as.numeric(coef(log_mod1)[7])))*100, 2)`% less likely to churn compared to inactive members. Since KNN classification does not indicate the importance of variables, further investigation may be required to obtain the most important predictor for the KNN model.

@fig-performance below shows a comparison of the two models after being tested with the hold-out set. The sensitivity is over 20% higher for the KNN model than the logistic regression model. On the other hand, the specificity is about 5% lower for the KNN model than the logistic regression model. Overall, the KNN classification model performed better than the logistic regression model since the miss-classification rate is lower and accuracy is higher than logistic regression model.

```{r fig-performance, ref.label="performance", output=TRUE, message=FALSE, fig.cap="Model performance comparison"}
```


## Conclusion

This study developed two models to predict customer churn at a bank. While the two models vary in accuracy, the bank can use them in different ways to improve customer churn rates.

The KNN model has a stronger accuracy rate than the logistic regression model and can therefore be used for prediction. The bank can use this model to predict what types of customers are more likely to churn at any given time and develop prevention strategies directed at these customers.

Although the logistic regression model is not as accurate as the KNN model, its ability to distinguish between predictors of importance could give leaders at the bank an idea of where to target first when developing business strategies. For example, promoting continuous product usage through promotional offers or advertisement may result in more active customers and reduce churn.


\newpage

## Supplementary material
### Data definitions for categorical variables
Here are the data definitions for categorical variables in the final data set. Definitions for the continuous variables remain the same as the original data source (@bankData).

* Exited: Whether the customer has a churned or not (0 = not churned, 1 = churned)
* Gender: The customer's gender (1 = female, 2 = male)
* Geography: Country where the customer lives (1 = France, 2 = Germany, 3 = Spain)
* Has credit card: Whether the customer has a credit card or not (0 = no, 1 = yes)
* Is active member: Whether the customer is active in the bank or not (0 = no, 1 = yes)

### Figures
```{r fig-stat-summary, ref.label="exploration-1", output=TRUE, message=FALSE, fig.cap="Summary of the original Kaggle data used prior to cleansing", fig.height=4, fig.width=3}
```

::: {#fig-boxplots layout-ncol=2}
```{r fig-boxplot-1, ref.label="boxplot-1", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
```{r fig-boxplot-2, ref.label="boxplot-2", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
```{r fig-boxplot-3, ref.label="boxplot-3", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
```{r fig-boxplot-4, ref.label="boxplot-4", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
```{r fig-boxplot-5, ref.label="boxplot-5", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
```{r fig-boxplot-6, ref.label="boxplot-6", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
Boxplots of continuous variables
:::

::: {#fig-barcharts layout-ncol=2}
```{r fig-barchart-1, ref.label="barchart-1", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
```{r fig-barchart-2, ref.label="barchart-2", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
```{r fig-barchart-3, ref.label="barchart-3", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
```{r fig-barchart-4, ref.label="barchart-4", output=TRUE, message=FALSE, fig.height=2, fig.width=3}
```
Bar charts of categorical variables
:::

```{r fig-corrplot, ref.label="corrplot", output=TRUE, message=FALSE, fig.cap="Correlation between continuous variables", fig.height=3.5, fig.width=3.5}
```

```{r fig-reg, ref.label="log-reg-2", output=TRUE, message=FALSE, fig.cap="Final logistic regression model"}
```

```{r fig-roc, ref.label="log-reg-3", output=TRUE, message=FALSE, fig.cap="ROC curve to determine cutoff", fig.height=3, fig.width=3}
```



### Code
```{r code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, size=11}
```

\newpage

## References

::: {#refs}
:::
